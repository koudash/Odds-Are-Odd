# Odds-Are-Odd
<p><strong>Is odds low enough trustworthy in predicting a home win? Shall I never go with high odds? What is the balance between risk and profit in soccer betting?</strong><p>
<img src="/static/image/ipynb/soccer_betting.jpg" alt="soccer betting"> 

<p> To address the aforementioned questions, odds movements data of Season 2018/2019 for top five European leagues (<i><strong>Premier League</strong></i> from England, <i><strong>La Liga</strong></i> from Spain, <i><strong>Bundesliga</strong></i> from Germany, <i><strong>Serie A</strong></i> from Italy, and <i><strong>Ligue1</strong></i> from France) as well as those for ongoing Season 2019 of <i><strong>MLS</strong></i> (U.S) were scraped from <a href="http://info.310win.com/cn/League/2018-2019/36.html">310win.com</a> using <code>Selenium</code>. Specifically, for each league, a randomly selected week of match was put aside for model testing purpose. All other weeks of matches were used to generate machine learning models. Match info. has been translated back to English during data processing.<p>

<p> Scraped data including columns of match week ("week"), names of home ("home") and away ("away") teams, betting company ("company") where the odds came from, odds for home team to win ("win_odds"), draw ("draw_odds"), and lose ("lose_odds") the match, time of odds apart from start of the match as calculated in minutes ("odds_delta_time"), and match result ("result"). In considering the trainings of machine learning (ML) models, all columns besides "result" were considered as features whereas the latter as target. It turned out that data in feature columns were not normally distributed, especially from the kurtosis point of view.</p>
<img src="/static/image/ipynb/feature_data_distr.png" alt="feature data distr"> 
<p>Correlation study of the feature columns further revealed that "lose_odds" was actually highly correlated with "draw_odds" (sometimes with "win_odds" as well).<p>
<img src="/static/image/ipynb/feature_corr.png" alt="feature corr">
<p>Target column ("result"), on the other hand, was tested for time distribution of the odds. Depending on the leagues, there actually existed a threshold time line, beyond which odds records were rare. For Premier League and Bundesliga, the window period is 21 days; for La Liga, Serie A, and Ligue1, it is 14 days; and for MLS, it is only 7 days. (It somewhat echoed the predicting performance of odds by the end of this project, seemingly the more familiar the betting company are with the league, the longer the window is, and the better their oddes in predicting match results are).</p>
<img src="/static/image/ipynb/odds_time_distr.png" alt="odds time distr">
<p>Taken all the above analyses into consideration, we did not use linear regression model in this project due to feature data distribution. The ML models trained here were <strong>Random Forest (RF)</strong>, <strong>Support Vector Machine (SVM)</strong>, <strong>K-Nearest Neighbors (KNN)</strong>, and <strong>Neural Network (NN)</strong>. Betting companies chosen here were <strong>Bet 365</strong>, <strong>William Hill</strong>, and <strong>Bwin</strong>. <strong>12Bet</strong>, headquartered in Philippines, was also selected in order to have some Asian views. We have feature cpunts of either 4 or 3 (excluding "loss_odds") and I have target either with all odds data or only with those within the window period. With that, I was able to run my first attempt to screen for ML models. Again, that is 6 (leagues) x 4 (betting company) x 2 (features) x 2 (target) x 4 (models) = 384 runs.</p>

<p>After more than 24h running, ML models with the best performances across all leagues were RF and KNN with the number of neighbors as 5. And 12Bet is the best betting company we might want to retreive odds movements data from to train our model. Removal of odds that were too "old" facilitates for two thirds of leagues in building up ML models. Dispite a high correlation with odds for other match result(s), retaining "lose_odds" as feature is beneficial in building up ML models.<p>
<img src="/static/image/ipynb/top_scored.png" alt="top scored">
<img src="/static/image/ipynb/testing_score.png" alt="testing score">

<p>I next fine tuned ML models with <code>GridSearchCV</code>. As can be told from the below analysis, <strong>RF</strong> models seemed to perform a little bit better both in <i>testing</i> and in <i>precision</i> and <i>recall</i> scores over <strong>KNN</strong>. Ideally I should be using RF models for prediction. However, since the size of saved RF models wey exceeded Github's maximum file uploading requirement, I instead used <strong>KNN</strong> model for demo in html.</p>
<img src="/static/image/ipynb/model_comparison_final.png" alt="model comparison final">

<p>Html was made to 1) link to <a href="http://info.310win.com/cn/League/2018-2019/36.html">310win.com</a> to showcase data web scraping; 2) link to <a href="https://public.tableau.com/profile/lei8768#!/vizhome/odds_data_process/DBMSModels-Final">Tableau Public</a> to demonstrate data process and analyses; and 3) open a new html for match prediction (for MLS, it is match prediction; for the other five European leagues, it is validation using the randomly selected week of matches from Season 2018/2019 that were saved earlier).</p>

<p>League logos on prediction html serve as form button to pass league info to Flask route, which info is then be used to make calls to either saved .csv file or designated webpages in <a href="http://info.310win.com/cn/League/2018-2019/36.html">310win.com</a> to freshly scrape odds data. Retrieved data are then used to predict match results. Such results are sent (render_template) back to prediction html as string to be retrieved by internal javascript. Match info. is eventually processed by external static javascript and appended on prediction html for display.</p>
<img src="/static/image/ipynb/prediction_example.png" alt="prediction example">

